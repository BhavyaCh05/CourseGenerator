
--Sample code to assessment based on mcq, fill in the blanks and short answers.
# Example user responses
user_mcq_answers = ["A", "B", "C", "D", "A", "B", "C", "D", "A", "B"]
user_fill_in_the_blank_answers = ["answer1", "answer2", "answer3", "answer4", "answer5", "wrong", "answer7", "answer8", "answer9", "answer10"]
user_short_answer_answers = ["answer1", "wrong", "answer3", "wrong", "answer5", "answer6", "wrong", "answer8", "answer9", "wrong"]

# Correct answers (for evaluation purposes)
mcq_correct_answers = ["A", "B", "C", "D", "A", "B", "C", "D", "A", "B"]
fill_in_the_blank_correct_answers = ["answer1", "answer2", "answer3", "answer4", "answer5", "answer6", "answer7", "answer8", "answer9", "answer10"]
short_answer_correct_answers = ["answer1", "answer2", "answer3", "answer4", "answer5", "answer6", "answer7", "answer8", "answer9", "answer10"]

# Evaluate the user answers
def evaluate_assessment(user_answers, correct_answers):
    evaluation = {
        'mcq': {
            'total': len(correct_answers['mcq']),
            'correct': sum(1 for ua, ca in zip(user_answers['mcq'], correct_answers['mcq']) if ua == ca)
        },
        'fill_in_the_blank': {
            'total': len(correct_answers['fill_in_the_blank']),
            'correct': sum(1 for ua, ca in zip(user_answers['fill_in_the_blank'], correct_answers['fill_in_the_blank']) if ua == ca)
        },
        'short_answer': {
            'total': len(correct_answers['short_answer']),
            'correct': sum(1 for ua, ca in zip(user_answers['short_answer'], correct_answers['short_answer']) if ua.strip().lower() == ca.strip().lower())
        }
    }
    return evaluation

user_answers = {
    'mcq': user_mcq_answers,
    'fill_in_the_blank': user_fill_in_the_blank_answers,
    'short_answer': user_short_answer_answers
}

correct_answers = {
    'mcq': mcq_correct_answers,
    'fill_in_the_blank': fill_in_the_blank_correct_answers,
    'short_answer': short_answer_correct_answers
}

# Evaluate user answers
evaluation = evaluate_assessment(user_answers, correct_answers)
print("Evaluation Results:")
print(evaluation)

# Calculate accuracy rates
accuracy_mcq = (evaluation['mcq']['correct'] / evaluation['mcq']['total']) * 100
accuracy_fill_in_the_blank = (evaluation['fill_in_the_blank']['correct'] / evaluation['fill_in_the_blank']['total']) * 100
accuracy_short_answer = (evaluation['short_answer']['correct'] / evaluation['short_answer']['total']) * 100

print(f"MCQ Accuracy: {accuracy_mcq}%")
print(f"Fill-in-the-Blank Accuracy: {accuracy_fill_in_the_blank}%")
print(f"Short Answer Accuracy: {accuracy_short_answer}%")









